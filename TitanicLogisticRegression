'''
SAMPLE LOGISTIC REGRESSION USING TITANIC DATA FROM KAGGLE:
Suvived is the Y variable being analyzed for this project.
'''
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns

#Import the data set
titanic_data = pd.read_csv(r'C:\Users\Scott\Desktop\titanic_train.csv')

#Drop all rows with missing data:
titanic_data.dropna(inplace=True)
#print(titanic_data)

#These values have no numeric value:
titanic_data.drop(['Name', 'PassengerId', 'Ticket', 'Sex', 'Embarked', 'Cabin'], axis = 1, inplace = True)


#Split the data set into x and y data
y_data = titanic_data['Survived']
#print(y_data)
x_data = titanic_data.drop('Survived', axis = 1)

#Split the data set into training data and test data
from sklearn.model_selection import train_test_split
x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x_data, y_data, test_size = 0.3)

#Create the model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

#Train the model and create predictions
model.fit(x_training_data, y_training_data)
predictions = model.predict(x_test_data)

#Calculate performance metrics
from sklearn.metrics import classification_report
print(classification_report(y_test_data, predictions))

#Generate a confusion matrix
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test_data, predictions))

'''
   precision    recall  f1-score   support

           0       0.80      0.17      0.29        23
           1       0.62      0.97      0.76        32

    accuracy                           0.64        55
   macro avg       0.71      0.57      0.52        55
weighted avg       0.70      0.64      0.56        55

[[ 4 19]
 [ 1 31]]

'''
