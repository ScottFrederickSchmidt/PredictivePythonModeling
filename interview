'''
Python data science interview coding project for a Fortune50 company!!
Step1: Clean and prepare your data:
The data in this exercise have been simulated to mimic real, dirty data. 
Please clean the data with whatever method(s) you believe to be best/most suitable. 
Success in this exercise typically involves feature engineering and avoiding data leakage. 
You may create new features. However, you may not add or supplement with external data. 
'''

import numpy as np
import pandas as pd
import seaborn as sns

'''
Data leakage can be defined as: 
1)Understanding the Dataset.
2)Cleaning Dataset for Duplicates.
3)Selecting Features with Regard to Target Variable Correlation and Temporal Ordering.
4)Splitting Dataset into Train, Validation, and Test Groups.
5)Normalizing After Splitting, BUT Before Cross Validation.
'''

#df=r'C:\Users\scot\Desktop\exercise_40_test' 
df=r'C:\Users\scott\Desktop\exercise_40_train.csv' 
df=pd.read_csv(df)
df.dropna() #delete any rows with missing values the simple way
df=df.drop_duplicates() #drop dupliates the smiple way
# print(df)

'''
Step 2 - Build your models: 
For this exercise, you are required to build two models. The first model must be a logistic regression. 
The second model may be any supervised learning algorithm that is not from the GLM family.
'''

#LOGISTIC REGRESSION:
from sklearn.model_selection import train_test_split # splitting the data
i=1

X = np.asarray(df[['x'+i]])
y = np.asarray(df['y'])

from sklearn.linear_model import LogisticRegression # model algorithm
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = None)

logDF=[]

for i in range(1, 10_000):
    logmodel=LogisticRegression()
    logDF=logmodel.fit(X_train, y_train)
    

'''
Step 3 - Generate predictions:
Create predictions on the data in test.csv using each of your trained models. 
The predictions should be the class probabilities for belonging to the positive class (labeled '1').  

Be sure to output a prediction for each of the rows in the test dataset (10K rows). 
Save the results of each of your models in a separate CSV file.  Title the two files 'glmresults.csv' and 'nonglmresults.csv'. 
Each file should have a single column representing the predicted probabilities for its respective model. \
Please do not include a header label or index column. 
'''

#index = false gets rid of index and header=False gets rid of header
#df.to_csv(r'C:\Users\Scott\Desktop\logDF.csv', index=False, header=False)
#df.to_csv(r'C:\Users\Scott\Desktop\test.csv', index=False, header=False)


'''
Step 4 - Compare the modeling approaches:
Please write an executive summary that includes a comparison of the two modeling approaches, with emphasis on  relative strengths and weaknesses of the algorithms.
To receive maximum points on the executive summary, at least one strength and one weakness for each algorithm should be described.
Additionally, your executive summary should include which algorithm you think will perform better on the test set, and your support for that decision. 
Based on your model development process, include estimates for the test AUCs for each model. 
The estimates should be in a table and rounded to four decimal places. 
Finally, describe how you would demonstrate to a business partner that one model is better than the other without using a scoring metric.
'''


'''
Step 5 - Submit your work: 
Your submission should consist of 
(a) all the code used for exploratory data analysis, cleaning, prepping, and modeling (text or pdf preferred);
(b) the two results files (.csv format - each containing 10,000 decimal probabilities); 
and (c) your short report comparing the pros and cons of the two modeling techniques used (text or pdf preferred). 
Note: The results files should only include the column of probabilities.

Your work will be evaluated in the following areas:
• The appropriateness of the steps you took
• The complexity of your models
• The performance of each model on the test set (using AUC)
• The organization and readability of your code
• The write-up comparing the models
'''
