'''
Python data science interview coding project for a Fortune50 company!
Step1: Clean and prepare your data:
The data in this exercise have been simulated to mimic real, dirty data. 
Please clean the data with whatever method(s) you believe to be best/most suitable. 
Success in this exercise typically involves feature engineering and avoiding data leakage. 
You may create new features. However, you may not add or supplement with external data. 
'''

import numpy as np
import pandas as pd
import time
import seaborn as sns

'''
Data leakage can be defined as: 
1)Understanding the Dataset.
2)Cleaning Dataset for Duplicates.
3)Selecting Features with Regard to Target Variable Correlation and Temporal Ordering.
4)Splitting Dataset into Train, Validation, and Test Groups.
5)Normalizing After Splitting, BUT Before Cross Validation.
'''

start=time.time()

#df=r'C:\Users\scot\Desktop\exercise_40_test' 
df=r'C:\Users\scott\Desktop\exercise_40_train.csv' 
df=pd.read_csv(df)
df=df.dropna() #delete any rows with missing values the simple way 
#Note: A more complex way for missing data is to use a function to impute missing values by taking the mean of a specific column.

df=df.drop_duplicates() #drop dupliates the smiple way
# print(df)

'''
Step 2 - Build your models: 
For this exercise, you are required to build two models. The first model must be a logistic regression. 
The second model may be any supervised learning algorithm that is not from the GLM family.
'''

#LOGISTIC REGRESSION:
from sklearn.model_selection import train_test_split # splitting the data
from sklearn.linear_model import LogisticRegression # model algorithm
from sklearn import metrics
import matplotlib.pyplot as plt

y = np.asarray(df['y']) #This will ALWAYS stay the same as the depedent variable column.
logDF=[] #keep track of the logistic regression results

for i in range(1, 101):
    xNUM='x'+str(i) #this will increment through column x1 to x100 as new independent variable.
    try:
        X = np.asarray(df[[xNUM]]) #this will increment through x1 to x100. 
        logmodel=LogisticRegression()
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = None)
        
        #instantiate the model
        log_regression = LogisticRegression()

        #fit the model using the training data
        log_regression.fit(X_train,y_train)

        #use model to make predictions on test data
        y_pred = log_regression.predict(X_test)
        
        cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
        probability= round(metrics.accuracy_score(y_test, y_pred), 4)

        logDF.append(probability) #add probability to list
      
        
        time.sleep(.05) #DO NOT WANT TO CRASH MY NEW COMPUTER!
    except:
        #This WILL sometimes happen, for example on 'Tuesday':
        print("An ERROR occurred on: ", xNUM)
        logDF.append("ERORR") 
print("Done calculating logistic regression test train in: ",  round(time.time()-start, 3),, " seconds")

'''
Step 3 - Generate predictions:
Create predictions on the data in test.csv using each of your trained models. 
The predictions should be the class probabilities for belonging to the positive class (labeled '1').  

Be sure to output a prediction for each of the rows in the test dataset (10K rows). 
Save the results of each of your models in a separate CSV file.  Title the two files 'glmresults.csv' and 'nonglmresults.csv'. 
Each file should have a single column representing the predicted probabilities for its respective model. \
Please do not include a header label or index column. 

NOTES:
#index  = false gets rid of index 
#header = false gets rid of header
'''

# Create the pandas logistic regression pandas DataFrame:
logDF = pd.DataFrame(logDF)
logDF.to_csv(r'C:\Users\Scott\Desktop\logDF.csv', index=False, header=False)

# Create the other pandas DataFrame:
nonDF = pd.DataFrame(df)
nonDF.to_csv(r'C:\Users\Scott\Desktop\nonglmresults.csv', index=False, header=False)

print("Program finishing! ",  round(time.time()-start, 3), " seconds.")
