'''
Decision Tree classification predictive model for a fortune50 coding interview:
The first column 'y' is the independent variable being analyzed for this project.
'''

#Numerical computing libraries
import pandas as pd
import numpy as np

#Visalization libraries
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

#Import the data set
df=r'C:\Users\scott\Desktop\exercise_40_train.csv' 
df=pd.read_csv(df)

#Exploratory data analysis
info=df.info()
pairplot=df.pairplot(df, hue = 'y')

''' STEP1: CLEAN THE DATA '''

#Drop columns that provide no numberic value:
df.drop(['x3', 'x7', 'x19',  'x24', 'x30', 'x31', 'x33', 'x39',
         'x42', 'x44', 'x49', 'x52', 'x57', 'x58', 'x60', 'x65', 
         'x67', 'x77', 'x93', 'x99'], axis = 1, inplace = True)

df.dropna(inplace=True) #delete any rows with missing values the simple way.

''' STEP2: TRAIN THE DATA '''

#Split the data set into training data and test data
from sklearn.model_selection import train_test_split
x = df.drop('y', axis = 1)
y = df['y']
x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x, y, test_size = 0.3)

#Train the decision tree model
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
model.fit(x_training_data, y_training_data)
predictions = model.predict(x_test_data)

''' STEP3: PREDICT THE DATA '''

#Measure the performance of the decision tree model
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, r2_score
print(classification_report(y_test_data, predictions))
print(confusion_matrix(y_test_data, predictions))

#Train the random forests model
from sklearn.ensemble import RandomForestClassifier
random_forest_model = RandomForestClassifier()
random_forest_model.fit(x_training_data, y_training_data)
random_forest_predictions = random_forest_model.predict(x_test_data)

#Measure the performance of the random forest model
print(classification_report(y_test_data, random_forest_predictions))
print(confusion_matrix(y_test_data, random_forest_predictions))



'''
#FINAL RESULTS THAT WERE PRINTED:
    precision    recall  f1-score   support

           0       0.93      0.74      0.83        35
           1       0.10      0.33      0.15         3

    accuracy                           0.71        38
   macro avg       0.51      0.54      0.49        38
weighted avg       0.86      0.71      0.77        38

[[26  9]
 [ 2  1]]
              precision    recall  f1-score   support

           0       0.92      1.00      0.96        35
           1       0.00      0.00      0.00         3

    accuracy                           0.92        38
   macro avg       0.46      0.50      0.48        38
weighted avg       0.85      0.92      0.88        38

[[35  0]
 [ 3  0]]
'''
